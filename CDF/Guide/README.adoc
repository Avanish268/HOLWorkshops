link:#pre-requisites[*Pre-requisites*]

link:++#lab-0---introduction-and-setup++[*Lab 0 - Introduction and setup*]

* link:#verify-access-to-the-workshop-environment[1. Verify access to the workshop environment]

* link:#verify-permissions-in-apache-ranger[2. Verify permissions in Apache Ranger]

** link:#accessing-apache-ranger[2.1 Accessing Apache Ranger]

** link:#kafka-permissions[2.2 Kafka Permissions]

** link:#schema-registry-permissions[2.3 Schema Registry Permissions]

* link:#update-workload-password[3. Update workload password]

* link:#obtain-the-kafka-broker-list[4. Obtain the Kafka Broker List]

** link:#step-1-access-the-data-hub[Step 1 : Access the Data Hub]

** link:#step-2-go-to-the-streams-messaging-interface[Step 2 : Go to the Streams Messaging Interface]

** link:#step-3-select-brokers-from-the-left-tab[Step 3 : Select Brokers from the left tab]

** link:#step-4-save-the-broker-list[Step 4 : Save the broker list]

* link:#download-resources-from-github[5. Download Resources from GitHub]

** link:#step-1-access-the-url-shared-by-the-instructor-for-github[Step 1 : Access the URL shared by the instructor for GitHub]

** link:#step-2-download-the-repo-as-a-zip-file[Step 2 : Download the repo as a zip file]

** link:#step-3-uncompress-the-files[Step 3 : Uncompress the Files]

* link:#unlock-your-keytab[6. Unlock your KeyTab]

** link:#unlock-your-keytab-if-it-is-not-unlocked-already[1. Unlock your Keytab if it is not unlocked already]

*** link:#step-1-go-to-the-ssb-data-hub[Step 1 : Go to the SSB Data Hub]

*** link:#step-2-open-the-ssb-ui-by-clicking-on-streaming-sql-console[Step 2 : Open the SSB UI by clicking on Streaming SQL Console]

*** link:#step-3-click-on-the-user-name-at-the-bottom-left-of-the-screen-and-select-manage-keytab[Step 3 : Click on the User name at the bottom left of the screen and select Manage Keytab]

*** link:#step-4-enter-your-workload-username-wuserxx-and-password.[Step 4 : Enter your Workload Username (wuserxx) and Password.]

*** link:#step-5-click-on-unlock-keytab[Step 5 : Click on unlock KeyTab]

** link:#reset-your-keytab-if-it-is-already-unlocked[2. Reset your KeyTab if it is already unlocked]

*** link:#step-1-go-to-the-ssb-data-hub-1[Step 1 : Go to the SSB Data Hub]

*** link:#step-2-open-the-ssb-ui-by-clicking-on-streaming-sql-console-1[Step 2 : Open the SSB UI by clicking on Streaming SQL Console]

*** link:#step-3-click-on-the-user-name-at-the-bottom-left-of-the-screen-and-select-manage-keytab-1[Step 3 : Click on the User name at the bottom left of the screen and select Manage Keytab]


link:#lab-1-create-a-flow-using-the-flow-designer[*Lab 1 : Create a Flow using the Flow Designer*]


* link:#overview[1. Overview]

* link:#building-the-data-flow[2. Building the Data Flow]

** link:#create-the-canvas-to-design-your-flow[2.1. Create the canvas to design your flow]

*** link:#step-1-access-the-dataflow-data-service[Step 1: Access the DataFlow Data Service]

*** link:#step-2-go-to-the-flow-design[Step 2: Go to the Flow Design ]

*** link:#step-3-create-a-new-draft[Step 3: Create a new Draft ]

*** link:#step-4-select-the-appropriate-environment[Step 4: Select the appropriate environment ]

** link:#adding-new-parameters[2.2. Adding new parameters ]

*** link:#step-1-click-on-the-flow-options-on-the-top-right-corner-of-your-canvas-and-then-select-parameters[Step 1: Click on the FLOW OPTIONS on the top right corner of your canvas and then select PARAMETERS ]

*** link:#step-2-configure-parameters[Step 2: Configure Parameters ]

** link:#create-the-flow[2.3. Create the Flow ]

*** link:#step-1-add-generateflowfile-processor[Step 1: Add GenerateFlowFile processor ]

*** link:#step-2-configure-generateflowfile-processor[Step 2: Configure GenerateFlowFile processor ]

*** link:#step-3-add-putcdpobjectstore-processor[Step 3: Add PutCDPObjectStore processor ]

*** link:#step-4-configure-putcdpobjectstore-processor[Step 4: Configure PutCDPObjectStore processor ]

*** link:#step-5-create-connection-between-processors[Step 5: Create connection between processors ]

** link:#naming-the-queues[2.4. Naming the queues ]

* link:#testing-the-data-flow[3. Testing the Data Flow ]

** link:#step-1-start-test-session[Step 1: Start test session ]

** link:#step-2-run-the-flow[Step 2: Run the flow ]

* link:#move-the-flow-to-the-flow-catalog[4. Move the Flow to the Flow Catalog ]

** link:#step-1-stop-the-current-test-session[Step 1: STOP the current test session ]

** link:#step-2-publish-the-flow[Step 2: PUBLISH the flow ]

** link:#step-3-give-your-flow-a-name-and-click-on-publish[Step 3: Give your flow a name and click on PUBLISH ]

* link:#deploying-the-flow[5. Deploying the Flow 47]

** link:#step-1-search-for-the-flow-in-the-flow-catalog[Step 1: Search for the flow in the Flow Catalog ]

** link:#step-2-deploy-the-flow[Step 2: Deploy the flow ]

** link:#step-3-select-the-cdp-environment[Step 3: Select the CDP environment ]

** link:#step-4-deployment-name[Step 4: Deployment Name ]

** link:#step-5-set-the-nifi-configuration[Step 5: Set the NiFi Configuration ]

** link:#step-6-set-the-parameters[Step 6: Set the Parameters ]

** link:#step-7-set-the-cluster-size[Step 7: Set the cluster size ]

** link:#step-8-add-key-performance-indicators[Step 8: Add Key Performance indicators ]

** link:#step-9-click-deploy[Step 9: Click Deploy ]

* link:#viewing-details-of-the-deployed-flow[6. Viewing details of the deployed flow ]

** link:#step-1-manage-kpi-and-alerts[Step 1 : Manage KPI and Alerts ]

** link:#step-2-manage-sizing-and-scaling[Step 2 : Manage Sizing and Scaling ]

** link:#step-3-manage-parameters[Step 3 : Manage Parameters ]

** link:#step-4-nifi-configurations[Step 4 : NiFi Configurations ]

** link:#step-5-view-the-deployed-flow-in-nifi[Step 5 : View the deployed flow in NiFi ]

** link:#step-6-terminate-the-flow[Step 6 : Terminate the flow ]

link:#lab-2-migrating-existing-data-flows-to-cdf-pc[Lab 2 : Migrating Existing Data Flows to CDF-PC]

* link:#overview-1[1. Overview ]

* link:#pre-requisites-1[2. Pre-requisites ]

** link:#create-a-kafka-topic[2.1. Create a Kafka Topic ]

** link:#create-a-schema-in-schema-registry[2.2. Create a Schema in Schema Registry ]


link:#lab-3-operationalizing-externally-developed-data-flows-with-cdf-pc[Lab 3 : Operationalizing Externally Developed Data Flows with CDF-PC]


* link:#import-the-flow-into-the-cdf-pc-catalog[1. Import the Flow into the CDF-PC Catalog ]

* link:#deploy-the-flow-in-cdf-pc[2. Deploy the Flow in CDF-PC ]


link:#lab-4-sql-stream-builder[Lab 4 : SQL Stream Builder]


* link:#overview-2[1. Overview ]

* link:#creating-a-project[2. Creating a Project ]

** link:#step-1-go-to-the-sql-stream-builder-ui[Step 1: Go to the SQL Stream Builder UI ]

** link:#step-2-creation-of-a-project[Step 2: Creation of a Project ]

** link:#step-3-create-kafka-data-store[Step 3 : Create Kafka Data Store ]

** link:#step-4-create-kafka-table[Step 4: Create Kafka Table ]

** link:#step-5-configure-the-kafka-table[Step 5: Configure the Kafka Table ]

** link:#step-6-create-a-flink-job[Step 6: Create a Flink Job]


== *Pre-requisites*

For the ease of carrying out the workshop and considering the time at hand, we have already taken care of some of the steps that need to be considered before we can start with the actual Lab steps. The prerequisites that need to be in place are:

[arabic]
. {blank}
+
____
Streams Messaging Data Hub Cluster should be created and running.
____
. {blank}
+
____
Stream analytics Data Hub cluster should be created and running.
____
. {blank}
+
____
Data provider should be configured in SQL Stream Builder.
____
. {blank}
+
____
Have access to the file syslog-to-kafka.json.
____
. {blank}
+
____
Environment should be enabled as part of the CDF Data Service.
____

Lab 0 basically talks about verifying different aspects wrt to access and connections before we could begin with the actual steps.

== 

== Lab 0 - Introduction and setup

=== 1. Verify access to the workshop environment

* {blank}
+
____
The *INSTRUCTOR* will share the Workshop link and the credentials before the start of the workshop
____
* {blank}
+
____
Open the shared link and login with the credentials assigned to you.
____

____
<Will be shared by the instructor at the start>

image:media/media/image76.png[media/media/image76,width=399,height=289]
____

* {blank}
+
____
You should land on the CDP Console as shown below.
____

image:media/media/image136.png[media/media/image136,width=520,height=241]

=== 

=== *2. Verify permissions in Apache Ranger*

==== 

____
NOTE: THESE STEPS HAVE ALREADY BEEN DONE FOR YOU, THIS SECTION WILL WALK YOU THROUGH HOW PERMISSIONS/POLICIES ARE MANAGED IN RANGER. PLEASE DO NOT EXECUTE THE STEPS IN THIS SECTION OR CHANGE ANYTHING.
____

==== *2.1 Accessing Apache Ranger*

____
Step 1 : Click on Management Console
____

image:media/media/image117.png[media/media/image117,width=456,height=213]

____
Step 2 : Click on Environments on the left tab

image:media/media/image30.png[media/media/image30,width=497,height=195]

Step 3 : Select the environment that is shared by the instructor and click on the *Ranger* quick link to access the Ranger UI

image:media/media/image61.png[media/media/image61,width=551,height=187]

image:media/media/image141.png[media/media/image141,width=551,height=286]
____

==== 

==== *2.2 Kafka Permissions*

[arabic]
. {blank}
+
____
In Ranger, select the Kafka repository that’s associated with the stream messaging datahub.image:media/media/image93.png[media/media/image93,width=390,height=169]
____
. {blank}
+
____
Verify if the user group(*workshop-users*) who will be performing the workshop is present in both *all-consumergroup* and *all-topic.*
____

____
image:media/media/image132.png[media/media/image132,width=580,height=177]
____

* {blank}
+
____
All-consumergroup
____

image:media/media/image109.png[media/media/image109,width=556,height=212]

* {blank}
+
____
all-topic
____

____
image:media/media/image128.png[media/media/image128,width=412,height=195]
____

==== 

==== 

==== *2.3 Schema Registry Permissions*

[arabic]
. {blank}
+
____
In Ranger, select the Schema Registry repository that’s associated with the stream messaging datahub.
____

____
image:media/media/image65.png[media/media/image65,width=479,height=105]
____

[arabic, start=2]
. {blank}
+
____
Verify if the user group(*workshop-users*) who will be performing the workshop is present in the Policy : *all - schema-group, schema-metadata, schema-branch, schema-version.*
____

____
image:media/media/image111.png[media/media/image111,width=549,height=191]

image:media/media/image83.png[media/media/image83,width=375,height=292]

image:media/media/image4.png[media/media/image4,width=550,height=96]
____

=== *3. Update workload password*

==== 

____
NOTE: THESE STEPS NEED TO BE PERFORMED BEFORE MOVING FORWARD
____

You will need to define your CDP Workload Password that will be used to access non-SSO interfaces. You may read more about it here. Please keep it with you. If you have forgotten it, you will be able to repeat this process and define another one.

* {blank}
+
____
Click on your user name (Ex: wuser00@workshop.com) at the lower left corner.
____

Click on Profile.

image:media/media/image153.png[media/media/image153,width=596,height=349]

* {blank}
+
____
Click option *Set Workload Password*.
____

image:media/media/image151.png[media/media/image151,width=514,height=209]

* {blank}
+
____
Enter the shared password.
____

==== 

____
NOTE: PLEASE ENTER THE SAME PASSWORD THAT WAS SHARED BY THE INSTRUCTOR. FAILING TO DO SO WILL LEAD TO ERRORS IN OUR LAB STEPS LATER ON

image:media/media/image41.png[media/media/image41,width=545,height=239]
____

* {blank}
+
____
Click the button Set Workload Password.
____

===  +

=== 

=== *4. Obtain the Kafka Broker List*

We will require the broker list to configure our processors to connect to our Kafka brokers which allow consumers to connect and fetch messages by partition, topic or offset.

This information can be found in the Data Hub cluster associated to the Streams Messaging Manager

==== Step 1 : Access the Data Hub

* {blank}
+
____
Go to the environment that is shared by the INSTRUCTOR
____

____
image:media/media/image51.png[media/media/image51,width=575,height=191]
____

* {blank}
+
____
Click on the Data Hub associated with Streams Messaging Manager (kafka-smm-cluster)
____

____
image:media/media/image17.png[media/media/image17,width=577,height=279]
____

==== Step 2 : Go to the Streams Messaging Interface

image:media/media/image75.png[media/media/image75,width=630,height=338]

==== Step 3 : Select Brokers from the left tab

image:media/media/image91.png[media/media/image91,width=630,height=280]

==== 

==== 

==== Step 4 : Save the broker list

== image:media/media/image43.png[media/media/image43,width=497,height=229]

Example :

kafka-smm-cluster-corebroker1.pko-hand.dp5i-5vkq.cloudera.site:9093

kafka-smm-cluster-corebroker0.pko-hand.dp5i-5vkq.cloudera.site:9093

kafka-smm-cluster-corebroker2.pko-hand.dp5i-5vkq.cloudera.site:9093

=== 

=== *5. Download Resources from GitHub*

==== Step 1 : Access the URL shared by the instructor for GitHub

image:media/media/image87.png[media/media/image87,width=404,height=273]

==== Step 2 : Download the repo as a zip file

image:media/media/image97.png[media/media/image97,width=418,height=280]

==== Step 3 : Uncompress the Files 

Uncompress the Files and you should have the following files and folders within it

==== image:media/media/image50.png[media/media/image50,width=349,height=78]

We will use this at a later point in our Labs

=== 

=== *6. Unlock your KeyTab*

To run queries on the SQL Stream Builder you need to have your KeyTab unlocked. This is mainly for authentication purposes. As the credential you are using is sometimes reused as part of other people doing the same lab it is possible that your Keytab is already unlocked. We have shared the steps for both the scenarios:

==== Unlock your Keytab if it is not unlocked already

===== Step 1 : Go to the SSB Data Hub

____
Click on Environments on the left tab and select the environment that is shared by the INSTRUCTOR

image:media/media/image30.png[media/media/image30,width=564,height=221]

Click on the DataHub associated with SQL Stream Builder (ssb-analytics-cluster)

image:media/media/image13.png[media/media/image13,width=544,height=276]
____

===== 

===== 

===== Step 2 : Open the SSB UI by clicking on *Streaming SQL Console*

image:media/media/image42.png[media/media/image42,width=559,height=237]

===== Step 3 : Click on the User name at the bottom left of the screen and select Manage Keytab

image:media/media/image16.png[media/media/image16,width=404,height=362]

==== 

===== 

===== Step 4 : Enter your Workload Username (wuserxx) and Password. 

image:media/media/image108.png[media/media/image108,width=328,height=218]

===== Step 5 : Click on unlock KeyTab

image:media/media/image52.png[media/media/image52,width=388,height=254]

image:media/media/image81.png[media/media/image81,width=292,height=123]

====  +

==== 

==== Reset your KeyTab if it is already unlocked

===== Step 1 : Go to the SSB Data Hub

____
Click on Environments on the left tab and select the environment that is shared by the INSTRUCTOR

image:media/media/image30.png[media/media/image30,width=564,height=221]

Click on the DataHub associated with SQL Stream Builder (ssb-analytics-cluster)

image:media/media/image13.png[media/media/image13,width=580,height=294]
____

===== 

===== 

===== Step 2 : Open the SSB UI by clicking on *Streaming SQL Console*

image:media/media/image42.png[media/media/image42,width=559,height=237]

===== image:media/media/image16.png[media/media/image16,width=319,height=285]

===== 

===== 

===== 

===== Step 3 : Click on the User name at the bottom left of the screen and select Manage Keytab

image:media/media/image45.png[media/media/image45,width=321,height=212]

If you get the following dialog box it means that your Keytab is already unlocked. *But it would be necessary to reset here by locking it and unlocking it again using your newly set workload password*

Step 3 : Enter your Principal Name which is the same as your workload username

____
*Example : wuserXY* image:media/media/image145.png[media/media/image145,width=271,height=174]
____

Click on Lock KeyTab

You can now continue from the STEP 3 in the “link:#unlock-your-keytab-if-it-is-not-unlocked-already[*_[.underline]#Unlock your KeyTab if not unlocked already#_*]” section above

== 

== *Lab 1 : Create a Flow using the Flow Designer*

____________________________________________________________________________

=== *1. Overview*

Creating a data flow for CDF-PC is the same process as creating any data flow within Nifi with 3 very important steps:

* {blank}
+
____
The data flow that would be used for CDF-PC must be self contained within a process group
____
* {blank}
+
____
Data flows for CDF-PC must use parameters for any property on a processor that is modifiable, e.g. user names, Kafka topics, etc.
____
* {blank}
+
____
All queues need to have meaningful names (instead of Success, Fail, and Retry). These names will be used to define Key Performance Indicators in CDF-PC.
____

The following is a step by step guide in building a data flow for use within CDF-PC.

=== *2. Building the Data Flow*

==== *2.1. Create the canvas to design your flow*

===== *Step 1:* Access the DataFlow Data Service

Access the DataFlow dataservice from the Management Console

image:media/media/image139.png[media/media/image139,width=406,height=231]

===== 

===== 

===== *Step 2:* Go to the Flow Design

image:media/media/image121.png[media/media/image121,width=143,height=270]

===== 

===== *Step 3: Create a new Draft* 

(This will be the main process group of your flow)

image:media/media/image124.png[media/media/image124,width=624,height=82]

===== *Step 4: Select the appropriate environment* 

*Select the appropriate environment as part of the workspace* and give your flow a name and click on *CREATE*

Workspace Name : *_The name of the environment will be shared by the INSTRUCTOR_*

image:media/media/image20.png[media/media/image20,width=361,height=212]

Draft Name : \{user_id}_datadump_flow

_Example : wuserXY_datadump_flow_

On successful creation of the Draft, you should now be redirected to the canvas on which you can design your flow

image:media/media/image125.png[media/media/image125,width=467,height=305]

==== 

==== 

==== *2.2. Adding new parameters*

===== *Step 1:* Click on the *FLOW OPTIONS* on the top right corner of your canvas and then select *PARAMETERS*

image:media/media/image130.png[media/media/image130,width=624,height=290]

===== *Step 2:* Configure Parameters

The next step is to configure what is called a parameter. These parameters are reused within the flow multiple times and will also be configurable at the time of deployment. Click on *ADD PARAMETER* to add non sensitive values, for any sensitive parameter please select *ADD SENSITIVE PARAMETER.*

image:media/media/image129.png[media/media/image129,width=450,height=207]

image:media/media/image138.png[media/media/image138,width=314,height=302]

We need to add the following parameters.

* {blank}
+
____
S3 Directory
____
** {blank}
+
____
Selection under Add Parameter : *_Add Parameter_*
____
** {blank}
+
____
Name : S3 Directory
____
** {blank}
+
____
Value : LabData or TestDir
____

____
image:media/media/image110.png[media/media/image110,width=330,height=310]
____

* {blank}
+
____
CDP Workload User
____
** {blank}
+
____
Selection under Add Parameter : *_Add Parameter_*
____
** {blank}
+
____
Name : CDP Workload User
____
** {blank}
+
____
Value : <The username assigned to you>
____
*** {blank}
+
____
EXAMPLE : wuser01
____
*** {blank}
+
____
IMPORTANT: do not add the domain ‘@workload.com’
____
* {blank}
+
____
CDP Workload User Password - [ Sensitive Field ]image:media/media/image103.png[media/media/image103,width=227,height=260]
____
** {blank}
+
____
Selection under Add Parameter : *_Add Sensitive Parameter_*
____
** {blank}
+
____
Name : CDP Workload User Password
____
** {blank}
+
____
Value : <Workload Password set by yourself in Lab 0>
____
*** {blank}
+
____
EXAMPLE : Wuser@2021image:media/media/image49.png[media/media/image49,width=298,height=281]
____

Click *APPLY CHANGES*

image:media/media/image126.png[media/media/image126,width=444,height=259]

Now go back to the Flow Designer. Click _‘Back to Flow Designer’_

image:media/media/image67.png[media/media/image67,width=470,height=203]

Now that we have created these parameters, we can easily search and reuse them within our dataflow. This is especially useful for *CDP Workload User* and *CDP Workload User Password*.

NOTE ONLY:

To search for existing parameters:

[arabic]
. {blank}
+
____
Open a processor's configuration and proceed to the properties tab.
____
. {blank}
+
____
Enter: *#\{*
____
. {blank}
+
____
Hit ‘control+spacebar’
____

This will bring up a list of existing parameters that are not tagged as sensitive.

==== 

==== *2.3. Create the Flow*

Let’s go back to the canvas to start designing our flow. This flow will contain 2 Processors:

* {blank}
+
____
*GenerateFlowFile* - Generates random data
____
* {blank}
+
____
*PutCDPObjectStore* - Loads data into HDFS(S3)
____
* {blank}
+
____
Our final flow will look like this:
____

image:media/media/image34.png[media/media/image34,width=267,height=353]

===== *Step 1:* Add *GenerateFlowFile* processor 

Pull the Processor onto the canvas and select *GenerateFlowFile* Processor and click on *ADD.*

image:media/media/image154.png[media/media/image154,width=496,height=330]

image:media/media/image3.png[media/media/image3,width=504,height=381]

===== 

===== *Step 2:* Configure *GenerateFlowFile* processor 

The GenerateFlowFile Processor will now be on your canvas and you can configure it in the following way by right clicking and selecting *Configuration*.

image:media/media/image25.png[media/media/image25,width=308,height=278]

Configure the processor in the following way

[width="100%",cols="50%,50%",options="header",]
|===
|*Property* |*Value*
|*Processor Name* |DataGenerator
|*Scheduling Strategy(default)* |Timer Driven
|*Run Duration(default)* |0 ms
|*Run Schedule* |30 sec
|*Execution(default)* |All Nodes
|*Custom Text* |<26>1 2021-09-21T21:32:43.967Z host1.example.com application4 3064 ID42 [exampleSDID@873 iut="4" eventSource="application" eventId="58"] application4 has stopped unexpectedly
|===

_This represents a syslog out in RFC5424 format. Subsequent portions of this workshop will leverage this same syslog format._

image:media/media/image11.png[media/media/image11,width=314,height=494]image:media/media/image14.png[media/media/image14,width=300,height=249]

Click on *APPLY.*

===== 

===== 

===== *Step 3:* Add *PutCDPObjectStore* processor 

Pull a new Processor onto the canvas and select *PutCDPObjectStore* Processor and click on *ADD.*

image:media/media/image1.png[media/media/image1,width=475,height=317]

image:media/media/image26.png[media/media/image26,width=477,height=342]

===== 

===== *Step 4:* Configure *PutCDPObjectStore* processor 

The PutCDPObjectStore Processor needs to be configured as follows:

[width="100%",cols="50%,50%",options="header",]
|===
|*Property* |*Value*
|*Processor Name* |Move2S3
|*Scheduling Strategy(default)* |Timer Driven
|*Run Duration(default)* |0 ms
|*Run Schedule(default)* |0 sec
|*Execution(default)* |All Nodes
|*Directory* |#\{S3 Directory}
|*CDP Username* |#\{CDP Workload User}
|*CDP Password* |#\{CDP Workload User Password}
|*Auto Terminate Relationships:* |Check the “Terminate” box under “success”
|===

===== image:media/media/image78.png[media/media/image78,width=372,height=565]image:media/media/image114.png[media/media/image114,width=403,height=586]

*Click APPLY*

image:media/media/image59.png[media/media/image59,width=258,height=235]

===== 

===== *Step 5:* Create connection between processors

Connect the two processors by dragging the arrow from *DataGenerator* processor to the *Move2S3* processor and select on *SUCCESS* relation and click *ADD*

image:media/media/image31.png[media/media/image31,width=357,height=409]

image:media/media/image112.png[media/media/image112,width=319,height=183]

Your flow will now look something like this:

image:media/media/image9.png[media/media/image9,width=197,height=354]

The Move2S3 processor does not know what to do in case of a failure, let’s add a retry queue to it. This can be done by dragging the arrow on the processor outwards then back to itself, as below:image:media/media/image106.png[media/media/image106,width=352,height=343]

image:media/media/image28.png[media/media/image28,width=287,height=184]

==== 

==== 

==== image:media/media/image149.png[media/media/image149,width=362,height=337]

==== 

==== 

==== 

==== 

==== 

====  +

==== 

==== *2.4. Naming the queues*

Providing unique names to all queues is very important as they are used to define Key Performance Indicators upon which CDF-PC will auto-scale.

To name a queue, double-click the queue and give it a unique name. A best practice here is to start the existing queue name (i.e. success, failure, retry, etc…) and add the source and destination processor information.

For example, the success queue between *DataGenerator* and *Move2S3* is named *success_Move2S3.*

image:media/media/image96.png[media/media/image96,width=552,height=396]

The failure queue for *Move2S3* is named *failure_Move2S3*.

image:media/media/image137.png[media/media/image137,width=538,height=365]

=== 

===  +

=== 

=== *3. Testing the Data Flow*

==== *Step 1:* Start test session

To test your flow we need to first start the test session

Click on *FLOW OPTIONS* and then select *START* on TEST SESSION

image:media/media/image2.png[media/media/image2,width=624,height=358]image:media/media/image98.png[media/media/image98,width=328,height=394]

In the next window, click *START SESSION*

The activation should take about a couple of minutes. While this happens you will see this at the top right corner of your screen

image:media/media/image7.png[media/media/image7,width=503,height=102]

Once the Test Session is ready you will see the following message on the top right corner of your screen.

image:media/media/image33.png[media/media/image33,width=517,height=112]

==== 

==== *Step 2:* Run the flow

Right click on the empty part of the canvas and select START.

image:media/media/image37.png[media/media/image37,width=383,height=363]

Both the processors should now be in the START state.

image:media/media/image39.png[media/media/image39,width=381,height=363]

You will now see files coming into the folder which was specified as the Directory on the S3 bucket which is the Base data store for this environment.

image:media/media/image23.png[media/media/image23,width=357,height=231]

image:media/media/image36.png[media/media/image36,width=577,height=285]

{empty}[You will not be able to access this S3 bucket by your self but the instructor will show you where everyones data is moving to]

=== *4. Move the Flow to the Flow Catalog*

After the flow has been created and tested we can now PUBLISH the flow to the Flow Catalog

==== *Step 1:* STOP the current test session 

STOP the current test session by clicking on the green tab on top right and click *END*

image:media/media/image88.png[media/media/image88,width=448,height=275]

image:media/media/image143.png[media/media/image143,width=366,height=143]

==== *Step 2:* PUBLISH the flow

Once the session stops, click on *FLOW OPTION* on the top right corner of your screen and click on *PUBLISH*

image:media/media/image140.png[media/media/image140,width=624,height=358]

====  +

==== 

==== 

==== Step 3: Give your flow a name and click on *PUBLISH*

Flow Name : \{user_id}_datadump_flow

image:media/media/image46.png[media/media/image46,width=334,height=261]

The flow will now be visible on the *FLOW CATALOG* and is ready to be deployed

image:media/media/image5.png[media/media/image5,width=401,height=360]

=== *5. Deploying the Flow*

==== *Step 1:* Search for the flow in the Flow Catalog

image:media/media/image29.png[media/media/image29,width=390,height=277]

____
Click on the Flow, you should see the following:
____

image:media/media/image24.png[media/media/image24,width=466,height=226]

====  +

==== 

==== 

==== *Step 2:* Deploy the flow

____
Click on *Version 1*, you should see a *Deploy* Option appear shortly. Then click on *Deploy*.

image:media/media/image38.png[media/media/image38,width=364,height=235]
____

==== *Step 3:* Select the CDP environment 

Select the CDP environment where this flow will be deployed and click on *CONTINUE*

NOTE: THE NAME OF THE ENVIRONMENT WILL BE SHARED BY THE INSTRUCTOR

image:media/media/image35.png[media/media/image35,width=334,height=316]

==== *Step 4:* Deployment Nameimage:media/media/image44.png[media/media/image44,width=416,height=252]

____
Give the deployment a unique name( \{user_id}_flow_prod), then click Next.

Example : wuser01_flow_prod
____

*Click NEXT*

==== image:media/media/image133.png[media/media/image133,width=257,height=307]

==== 

==== *Step 5:* Set the NiFi Configuration

We can let everything be the default here and click NEXT

==== 

==== *Step 6:* Set the Parametersimage:media/media/image147.png[media/media/image147,width=369,height=401]

Set the Username, Password and the Directory name and click NEXT

*CDP Workload User:* wuserXY

*CDP Workload User Password:* Wuser@2021

*S3 Directory:* dirFlowCatalogDataDump

*CDP Environment :* DummyParameter

_[The CDP Environment parameter that shows here is used at the time we perform a test run on our test session. It holds the CDP Environment configuration resources files such as ssl-client.xml, hive-site.xml and core-site.xml. You do not have to specify these to deploy your flow from the flow catalog as it automatically picks up those files,hence we give a dummy value to this. To avoid giving a dummy value, this parameter can be deleted before we publish the flow]_

==== image:media/media/image104.png[media/media/image104,width=278,height=331]

==== 

==== 

==== *Step 7:* Set the cluster size

Select the Extra Small size and click NEXT. In this step you can configure how your flow will autoscale, but keep it disabled for this lab.

==== 

==== 

==== 

==== *Step 8:* Add Key Performance indicators

Set up KPIs to track specific performance metrics of a deployed flow.

Click on “Add New KPI”

image:media/media/image22.png[media/media/image22,width=624,height=209]

In the KPI Scope drop-down list, choose “Connection”

image:media/media/image18.png[media/media/image18,width=332,height=368]

In the “Add New KPI” window, add an alert as below

image:media/media/image15.png[media/media/image15,width=357,height=393]

Click Add and then Click Next

image:media/media/image32.png[media/media/image32,width=351,height=409]

==== 

==== 

==== *Step 9:* Click *Deploy*

image:media/media/image60.png[media/media/image60,width=359,height=335]

The “Deployment Initiated” message will be displayed. Wait until the flow deployment is completed, which might take a few minutes.

image:media/media/image66.png[media/media/image66,width=580,height=276]

When deployed, the flow will show up on the Data flow dashboard, as below:

image:media/media/image8.png[media/media/image8,width=624,height=242]

=== *6. Viewing details of the deployed flow*

Click on the flow in the Dashboard and select Manage Deployment

image:media/media/image116.png[media/media/image116,width=496,height=324]

==== Step 1 : Manage KPI and Alerts

Click on the KPI tab to get the list of KPIs that have been set. You also have an option to modify or add more KPIs to your flow here.

image:media/media/image101.png[media/media/image101,width=630,height=369]

==== Step 2 : Manage Sizing and Scaling

Click on the Sizing and Scaling tab to get detailed information

image:media/media/image84.png[media/media/image84,width=470,height=287]

==== 

==== Step 3 : Manage Parameters

The parameters that we earlier created can be managed from the Parameters tab. Click on Parameters.

image:media/media/image118.png[media/media/image118,width=491,height=246]

==== Step 4 : NiFi Configurations

If you have set any configuration wrt to Nifi they will show up on the ‘NiFi Configuration’ tab

==== image:media/media/image86.png[media/media/image86,width=630,height=185]

====  +

==== 

==== Step 5 : View the deployed flow in NiFi

* {blank}
+
____
Select ACTIONS on the Deployment Manager page and then click on ‘View in NiFi’
____

____
image:media/media/image120.png[media/media/image120,width=560,height=313]

This will open the flow in the NiFi UI.

image:media/media/image95.png[media/media/image95,width=546,height=320]
____

====  +

==== 

==== Step 6 : Terminate the flow

As we have completed the Lab, it is best to terminate this flow. Follow the below given procedure to terminate your flow.

image:media/media/image131.png[media/media/image131,width=176,height=323]

Select Dashboard from the Cloudera Data Flow UI

Select your flow and go to Manage Deployment

== image:media/media/image115.png[media/media/image115,width=566,height=244] +

On the Deployment Manager Page, Select *Actions* and click on *Terminate*

image:media/media/image71.png[media/media/image71,width=630,height=181]

In the next dialog box, enter the name of the flow we are trying to terminate and click on *Terminate*

image:media/media/image105.png[media/media/image105,width=328,height=187]

You will now see that the termination process has started.

image:media/media/image152.png[media/media/image152,width=365,height=353]

== 

== *Lab 2 : Migrating Existing Data Flows to CDF-PC*

=== *1. Overview*

The purpose of this workshop is to demonstrate how existing NiFi flows can be migrated to the Data Flow Experience. This workshop will leverage an existing NiFi flow template that has been designed with the best practices for CDF-PC flow deployment.

The existing NiFi Flow will perform the following actions:

[arabic]
. {blank}
+
____
Generate random syslogs in 5424 Format
____
. {blank}
+
____
convert the incoming data to a JSON using record writers
____
. {blank}
+
____
Apply a SQL filter to the JSON records
____
. {blank}
+
____
Send the transformed syslog messages to Kafka
____

Note that a parameter context has already been defined in the flow and the queues have been uniquely named.

For this we will be leveraging the DataHubs which have already been created, namely:

* {blank}
+
____
ssb-analytics-cluster
____
* {blank}
+
____
kafka-smm-cluster
____

=== 

=== *2. Pre-requisites*

==== 2.1. Create a Kafka Topic

[arabic]
. {blank}
+
____
Login to Streams Messaging Manager by clicking the appropriate hyperlink in the Streams Messaging Datahub ( kafka-smm-cluster )
____

image:media/media/image92.png[media/media/image92,width=568,height=231]

image:media/media/image63.png[media/media/image63,width=563,height=86]

[arabic, start=2]
. {blank}
+
____
Click on Topics in the left tab
____

image:media/media/image148.png[media/media/image148,width=376,height=296]

[arabic, start=3]
. {blank}
+
____
Click on Add New
____

____
image:media/media/image122.png[media/media/image122,width=533,height=270]
____

[arabic, start=4]
. {blank}
+
____
Create a Topic with the following parameters then click *Save*:
____

* {blank}
+
____
*Name*: <username>-syslog
____
* {blank}
+
____
*Partitions*: 1
____
* {blank}
+
____
*Availability*: Moderate
____
* {blank}
+
____
*Cleanup Policy*: Delete
____

image:media/media/image64.png[media/media/image64,width=401,height=375]

____
*Note*: The Flow will not work if you set the Cleanup Policy to anything other than *Delete*. This is because we are not specifying keys when writing to Kafka.
____

==== 2.2. Create a Schema in Schema Registry

[arabic]
. {blank}
+
____
Login to Schema Registry by clicking the appropriate hyperlink in the Streams Messaging Datahub(kafka-smm-cluster )
____

image:media/media/image92.png[media/media/image92,width=541,height=220]

image:media/media/image72.png[media/media/image72,width=564,height=85]

[arabic, start=2]
. {blank}
+
____
Click on the + button on the top right to create a new schema.
____

image:media/media/image74.png[media/media/image74,width=451,height=187]

[arabic, start=3]
. {blank}
+
____
Create a new schema with the following information:
____

* {blank}
+
____
*Name*: <username>-syslog
____
* {blank}
+
____
*Description*: syslog schema for dataflow workshop
____
* {blank}
+
____
*Type*: Avro schema provider
____
* {blank}
+
____
*Schema Group*: Kafka
____
* {blank}
+
____
*Compatibility*: Backward
____
* {blank}
+
____
*Evolve*: True
____
* {blank}
+
____
*Schema Text*: Copy and paste the schema text below into the “Schema Text” field
____

[width="100%",cols="100%",options="header",]
|===
a|
\{

"name": "syslog",

"type": "record",

"namespace": "com.cloudera",

"fields": [

\{

"name": "priority",

"type": "int"

},

\{

"name": "severity",

"type": "int"

},

\{

"name": "facility",

"type": "int"

},

\{

"name": "version",

"type": "int"

},

\{

"name": "timestamp",

"type": "long"

},

\{

"name": "hostname",

"type": "string"

},

\{

"name": "body",

"type": "string"

},

\{

"name": "appName",

"type": "string"

},

\{

"name": "procid",

"type": "string"

},

\{

"name": "messageid",

"type": "string"

},

\{

"name": "structuredData",

"type": \{

"name": "structuredData",

"type": "record",

"fields": [

\{

"name": "SDID",

"type": \{

"name": "SDID",

"type": "record",

"fields": [

\{

"name": "eventId",

"type": "string"

},

\{

"name": "eventSource",

"type": "string"

},

\{

"name": "iut",

"type": "string"

}

]

}

}

]

}

}

]

}

|===

____
Note: The name of the Kafka Topic you previously created and the Schema Name must be the same.

Click on *SAVE*.

image:media/media/image142.png[media/media/image142,width=461,height=287]

image:media/media/image127.png[media/media/image127,width=435,height=124]
____

== 

==  +

== 

== *Lab 3 : Operationalizing Externally Developed Data Flows with CDF-PC*

=== *1. Import the Flow into the CDF-PC Catalog*

[loweralpha]
. {blank}
+
____
Open the CDF-PC data service and click on Catalog in the left tab.
____

____
image:media/media/image62.png[media/media/image62,width=161,height=176]
____

[loweralpha, start=2]
. {blank}
+
____
Select Import Flow Definition on the Top Right
____

____
image:media/media/image56.png[media/media/image56,width=191,height=37]
____

[loweralpha, start=3]
. {blank}
+
____
Add the following information:
____

* {blank}
+
____
*Flow Name:* <username>-syslog-to-kafka
____
* {blank}
+
____
*Flow Description:*
____

[width="100%",cols="100%",options="header",]
|===
a|
____
Reads Syslog in RFC 5424 format, applies a SQL filter, transforms the data into JSON records, and publishes to Kafka
____

|===

* {blank}
+
____
*NiFi Flow Configuration:* syslog-to-kafka.json (From the resources downloaded earlier)
____
* {blank}
+
____
*Version Comments:* Initial Version
____

image:media/media/image73.png[media/media/image73,width=242,height=248]

Click *IMPORT*

=== *2. Deploy the Flow in CDF-PC*

[arabic]
. {blank}
+
____
Search for the flow in the Flow Catalog
____

image:media/media/image58.png[media/media/image58,width=550,height=103]

[arabic, start=2]
. {blank}
+
____
Click on the Flow, you should see the following:
____

image:media/media/image55.png[media/media/image55,width=346,height=225]

[arabic, start=3]
. {blank}
+
____
Click on *Version 1*, you should see a *Deploy* Option appear shortly. Then click on *Deploy*.
____

____
image:media/media/image38.png[media/media/image38,width=364,height=235]
____

[arabic, start=4]
. {blank}
+
____
Select the CDP environment where this flow will be deployed, then click *Continue*.
____

NOTE: THE NAME OF THE ENVIRONMENT WILL BE SHARED BY THE INSTRUCTOR

image:media/media/image40.png[media/media/image40,width=489,height=359]

[arabic, start=5]
. {blank}
+
____
Give the deployment a unique name, then click *Next*.
____

____
Example : \{user_id}-syslog-to-kafka
____

image:media/media/image47.png[media/media/image47,width=468,height=88]

[arabic, start=6]
. {blank}
+
____
In the NiFi Configuration screen, click *Next*.
____

image:media/media/image79.png[media/media/image79,width=431,height=346]

[arabic, start=7]
. {blank}
+
____
Add the Flow Parameters as below, then click *Next*.
____

* {blank}
+
____
*CDP Workload User* - The workload username for the current user
____
** {blank}
+
____
Example : wuser00
____
* {blank}
+
____
*CDP Workload Password* - The workload password for the current user [This password was set by you in Lab 0, section 3]
____
* {blank}
+
____
*Filtre Rule -* SELECT * FROM FLOWFILE
____
* {blank}
+
____
*Kafka Broker Endpoint* - A comma separated list of Kafka Brokers.
____

{empty}[Obtained in Lab 0, section 4]

Example:

____
kafka-smm-cluster-corebroker1.pko-hand.dp5i-5vkq.cloudera.site:9093,kafka-smm-cluster-corebroker0.pko-hand.dp5i-5vkq.cloudera.site:9093,kafka-smm-cluster-corebroker2.pko-hand.dp5i-5vkq.cloudera.site:9093
____

* {blank}
+
____
*Kafka Destination Topic -* <username>-syslog (Ex: wuser00-syslog)
____
* {blank}
+
____
_*Kafka Producer ID* - nifi_dfx_p1_
____
* {blank}
+
____
*Schema Name* - <username>-syslog (Ex: wuser00-syslog)
____
* {blank}
+
____
*Schema Registry Hostname* - The hostname of the master server in the Kafka Datahub(kafka-smm-cluster)[Refer screenshot below]image:media/media/image135.png[media/media/image135,width=448,height=289]
____

____
_Example : kafka-smm-cluster-master0.pko-hand.dp5i-5vkq.cloudera.site_
____

.image:media/media/image69.png[media/media/image69,width=349,height=361]image:media/media/image19.png[media/media/image19,width=362,height=260]

[arabic, start=8]
. {blank}
+
____
On the next page, define the Sizing and Scaling as follows, then click *Next*.
____

* {blank}
+
____
*Size:* Extra Small
____
* {blank}
+
____
*Enable Auto Scaling:* True
____
* {blank}
+
____
*Min Nodes:* 1
____
* {blank}
+
____
*Max Nodes:* 3
____

____
image:media/media/image113.png[media/media/image113,width=325,height=228]
____

[arabic, start=9]
. {blank}
+
____
Skip the KPI page by clicking *Next* and Review your deployment. Then Click *Deploy*.
____

image:media/media/image146.png[media/media/image146,width=289,height=333]

[arabic, start=10]
. {blank}
+
____
Proceed to the CDF-PC Dashboard and wait for your flow deployment to complete, which might take a few minutes. A Green Check Mark will appear once complete, which might take a few minutes.
____

image:media/media/image48.png[media/media/image48,width=624,height=40]

[arabic, start=11]
. {blank}
+
____
Click into your deployment and then Click *Manage Deployment* on the top right to view your flow in NiFi.
____

____
image:media/media/image107.png[media/media/image107,width=515,height=287]
____

Now click on *ACTIONS* and select *_View in NiFi_*

image:media/media/image119.png[media/media/image119,width=555,height=289]

The flow that you just deployed will look something like this on NiFi

image:media/media/image144.png[media/media/image144,width=415,height=319]

Double click on the Process Group to see the flow

image:media/media/image150.png[media/media/image150,width=360,height=515]

== 

==  +

== 

== *Lab 4 : SQL Stream Builder*

=== *1. Overview*

The purpose of this workshop is to demonstrate streaming analytic capabilities using Cloudera SQL Stream Builder. We will leverage the NiFi Flow deployed in CDF-PC from the previous lab and demonstrate how to query live data and subsequently sink it to another location. The SQL query will leverage the existing syslog schema in Schema Registry.

=== *2. Creating a Project*

==== *Step 1:* Go to the SQL Stream Builder UI

SSB Interface can be reached from the Data Hub that is running the Streams Analytics, in our case - _ssb-analytics-cluster_

Within the Data Hub, click on *Streaming SQL Console*

image:media/media/image99.png[media/media/image99,width=630,height=278]

image:media/media/image6.png[media/media/image6,width=630,height=229]

==== *Step 2:* Creation of a Project

Create a SSB Project by clicking *“New Project”* using the following details and click “*Create”*

[.mark]#Name : \{user-id}_hol_workshop#

[.mark]#Description : SSB Project to analyze streaming data#

{empty}[.mark]##image:media/media/image80.png[media/media/image80,width=372,height=343]

Switch to the created project. Click on *Switch*

image:media/media/image94.png[media/media/image94,width=630,height=49]

==== 

==== 

==== *Step 3 :* Create Kafka Data Store 

Create Kafka Data Store by selecting “*Data Sources”* in the left pane, clicking on the three-dotted icon next to “*Kafka”*, then selecting *“New Kafka Data Source”*.

image:media/media/image68.png[media/media/image68,width=624,height=226]

*Name :* \{user-id}_cdp_kafka

*Brokers (Comma-separated List)*

kafka-smm-cluster-corebroker1.pko-hand.dp5i-5vkq.cloudera.site:9093,kafka-smm-cluster-corebroker0.pko-hand.dp5i-5vkq.cloudera.site:9093,kafka-smm-cluster-corebroker2.pko-hand.dp5i-5vkq.cloudera.site:9093image:media/media/image102.png[media/media/image102,width=450,height=399]

*Protocol :* SASL/SSL

*SASL Username :* <workload-username>

Example : wuserXY

*SASL Password :* <Set in Lab 0 Section 3>

*SASL Mechanism :* PLAIN

____
image:media/media/image12.png[media/media/image12,width=454,height=148]
____

Click on *VALIDATE* to test the connections once successful click on *CREATE*

image:media/media/image53.png[media/media/image53,width=319,height=108]

==== *Step 4:* Create Kafka Table

Create Kafka Table, by selecting *“Virtual Tables”* in the left pane, clicking on the three-dotted icon next to it, then clicking on *“New Kafka Table”*.

image:media/media/image77.png[media/media/image77,width=354,height=368]

==== 

==== *Step 5:* Configure the Kafka Table

[arabic]
. {blank}
+
____
Enter the following details in the Kafka Table dialog box:
____
* {blank}
+
____
Table Name: *\{user-id}_syslog_data*
____
* {blank}
+
____
Kafka Cluster: *<select the Kafka data source you created previously>*
____
* {blank}
+
____
Data Format: *JSON*
____
* {blank}
+
____
Topic Name: *<select the topic created in Schema Registry>*
____

____
image:media/media/image27.png[media/media/image27,width=307,height=256]
____

[arabic, start=2]
. {blank}
+
____
When you select Data Format as AVRO, you must provide the correct Schema Definition when creating the table for SSB to be able to successfully process the topic data. +
 +
For JSON tables, though, SSB can look at the data flowing through the topic and try to infer the schema automatically, which is quite handy at times. Obviously, there must be data in the topic already for this feature to work correctly. +
 +
*Note:* SSB tries its best to infer the schema correctly, but this is not always possible and sometimes data types are inferred incorrectly. You should always review the inferred schemas to check if it's correctly inferred and make the necessary adjustments. +
 +
Since you are reading data from a JSON topic, go ahead and click on *Detect Schema* to get the schema inferred. You should see the schema be updated in the *Schema Definition* tab.
____

____
image:media/media/image123.png[media/media/image123,width=421,height=280]
____

[arabic, start=3]
. {blank}
+
____
You will also notice that a "Schema is invalid" message appears upon the schema detection. If you hover the mouse over the message it shows the reason:
____

____
image:media/media/image54.png[media/media/image54,width=624,height=86]

You will fix this in the next step.
____

[arabic, start=4]
. {blank}
+
____
Each record read from Kafka by SSB has an associated timestamp column of data type TIMESTAMP ROWTIME. By default, this timestamp is sourced from the internal timestamp of the Kafka message and is exposed through a column called eventTimestamp. +
 +
However, if your message payload already contains a timestamp associated with the event (event time), you may want to use that instead of the Kafka internal timestamp. +
 +
In this case, the syslog message has a field called "*timestamp*" that contains the timestamp you should use. You want to expose this field as the table's "*event_time*" column. To do this, click on the Event Time tab and enter the following properties:
____
* {blank}
+
____
Use Kafka Timestamps: *Disable*
____
* {blank}
+
____
Input Timestamp Column: *timestamp*
____
* {blank}
+
____
Event Time Column: *event_time*
____
* {blank}
+
____
Watermark Seconds: *3*
____

____
image:media/media/image57.png[media/media/image57,width=542,height=213]
____

[arabic, start=5]
. {blank}
+
____
Now that you have configured the event time column, click on *Detect Schema* again. You should see the schema turn valid: +
image:media/media/image85.png[media/media/image85,width=202,height=62]
____
. {blank}
+
____
Click the *Create and Review* button to create the table.
____

____
image:media/media/image70.png[media/media/image70,width=449,height=270]

Review the table's DDL and click *Close*.
____

==== 

====  +

==== 

==== *Step 6:* Create a Flink Job

Create a Flink Job, by selecting *“Jobs”* in the left pane, clicking on the three-dotted icon next to it, then clicking on *“New Job”*.

image:media/media/image100.png[media/media/image100,width=431,height=294]

Give a job name and click *CREATE*

image:media/media/image82.png[media/media/image82,width=387,height=152]

The Query Editor should now show up

image:media/media/image10.png[media/media/image10,width=407,height=318]

____
Add the following SQL Statement in the Editor
____

[width="100%",cols="100%",options="header",]
|===
|SELECT * FROM *\{user-id}_syslog_data* WHERE severity <=3
|===

NOTE : Replace \{user-id} with your assigned username

____
Run the Streaming SQL Job by clicking *Execute*. Also, ensure your \{user_id}-syslog-to-kafka flow is running in CDF-PC.

image:media/media/image90.png[media/media/image90,width=624,height=242]
____

In the Results tab, you should see syslog messages with severity levels <=3

image:media/media/image21.png[media/media/image21,width=559,height=195]
